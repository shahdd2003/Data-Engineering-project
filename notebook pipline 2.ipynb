{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Read JSON from ADLS\").getOrCreate()\n",
    "\n",
    "# File path in Azure Data Lake (ADLS)\n",
    "file_path = \"abfss://###@#######.dfs.core.windows.net/PData_JS.json\"\n",
    "\n",
    "# Step 1: Read JSON file from Azure Data Lake Store (ADLS)\n",
    "df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "\n",
    "# Step 2: Convert DataFrame to JSON string\n",
    "json_data = df.toJSON().collect()[0]  # Collecting as a list and getting the first item\n",
    "\n",
    "# Step 3: Parse JSON data using Python's json module\n",
    "data = json.loads(json_data)\n",
    "\n",
    "# Step 4: Extract the 'output' list\n",
    "output_list = data.get('output', [])\n",
    "\n",
    "df=pd.DataFrame(output_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dict = json.loads(output_list[0]) \n",
    "\n",
    "# Now, data_dict is a dictionary and can be used to create the DataFrame:\n",
    "df = pd.DataFrame(data_dict) \n",
    "\n",
    "\n",
    "df.to_csv('abfss://###@#######.dfs.core.windows.net/PData_JS.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
